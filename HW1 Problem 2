import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Load dataset
file_path = "/home/sjim/Courses/Real Time ML/HW1/Housing.csv"
housing_df = pd.read_csv(file_path)

# Identify numerical and categorical features
num_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']
cat_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']

# Target variable
y = housing_df['price'].values.reshape(-1, 1)

# Standardize numerical features
scaler = StandardScaler()
X_num = scaler.fit_transform(housing_df[num_features])

# 2.a: Use only numerical features
X_train_num, X_val_num, y_train, y_val = train_test_split(X_num, y, test_size=0.2, random_state=42)

# One-hot encode categorical features for 2.b and 2.c
encoder = OneHotEncoder(sparse_output=False, drop='first')  # FIXED ERROR
X_cat = encoder.fit_transform(housing_df[cat_features])

# 2.b: Combine numerical and one-hot categorical features
X_full = np.hstack([X_num, X_cat])
X_train_full, X_val_full, _, _ = train_test_split(X_full, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
def prepare_tensors(X_train, X_val, y_train, y_val):
    return (
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(X_val, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32),
        torch.tensor(y_val, dtype=torch.float32),
    )

X_train_num_tensor, X_val_num_tensor, y_train_tensor, y_val_tensor = prepare_tensors(X_train_num, X_val_num, y_train, y_val)
X_train_full_tensor, X_val_full_tensor, _, _ = prepare_tensors(X_train_full, X_val_full, y_train, y_val)

# Create DataLoaders
batch_size = 32
def create_dataloader(X_train, y_train, X_val, y_val):
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    return train_loader, val_loader

train_loader_num, val_loader_num = create_dataloader(X_train_num_tensor, y_train_tensor, X_val_num_tensor, y_val_tensor)
train_loader_full, val_loader_full = create_dataloader(X_train_full_tensor, y_train_tensor, X_val_full_tensor, y_val_tensor)

# Define MLP Model
class MLPRegression(nn.Module):
    def __init__(self, input_size, hidden_sizes):
        super(MLPRegression, self).__init__()
        layers = []
        prev_size = input_size
        for h in hidden_sizes:
            layers.append(nn.Linear(prev_size, h))
            layers.append(nn.ReLU())
            prev_size = h
        layers.append(nn.Linear(prev_size, 1))
        self.model = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.model(x)

# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):
    train_losses, val_losses = [], []
    best_val_loss = float('inf')
    best_model_state = None

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * X_batch.size(0)

        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item() * X_batch.size(0)

        val_loss /= len(val_loader.dataset)
        val_losses.append(val_loss)

        # Save best model state
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict()

        print(f"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")

    return train_losses, val_losses, best_val_loss, best_model_state

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2.a: Train MLP with only numerical features
model_2a = MLPRegression(input_size=X_train_num.shape[1], hidden_sizes=[64, 32, 16]).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model_2a.parameters(), lr=0.001)
print("\nTraining Model 2.a (Numeric Features Only)...")
train_losses_2a, val_losses_2a, best_val_loss_2a, best_model_state_2a = train_model(
    model_2a, train_loader_num, val_loader_num, criterion, optimizer, num_epochs=100
)
torch.save(best_model_state_2a, "/home/sjim/Courses/Real Time ML/HW1/mlp_regression_2a.pth")

# 2.b: Train MLP with one-hot encoding
model_2b = MLPRegression(input_size=X_train_full.shape[1], hidden_sizes=[64, 32, 16]).to(device)
optimizer = optim.Adam(model_2b.parameters(), lr=0.001)
print("\nTraining Model 2.b (With One-Hot Encoding)...")
train_losses_2b, val_losses_2b, best_val_loss_2b, best_model_state_2b = train_model(
    model_2b, train_loader_full, val_loader_full, criterion, optimizer, num_epochs=100
)
torch.save(best_model_state_2b, "/home/sjim/Courses/Real Time ML/HW1/mlp_regression_2b.pth")

# 2.c: Train MLP with increased complexity
model_2c = MLPRegression(input_size=X_train_full.shape[1], hidden_sizes=[128, 64, 32, 16]).to(device)
optimizer = optim.Adam(model_2c.parameters(), lr=0.001)
print("\nTraining Model 2.c (Increased Complexity)...")
train_losses_2c, val_losses_2c, best_val_loss_2c, best_model_state_2c = train_model(
    model_2c, train_loader_full, val_loader_full, criterion, optimizer, num_epochs=100
)
torch.save(best_model_state_2c, "/home/sjim/Courses/Real Time ML/HW1/mlp_regression_2c.pth")

# Plot results
def plot_results(train_losses, val_losses, title):
    plt.figure(figsize=(8, 5))
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("MSE Loss")
    plt.title(title)
    plt.legend()
    plt.show()

plot_results(train_losses_2a, val_losses_2a, "Training vs Validation Loss (2.a - Numeric Features Only)")
plot_results(train_losses_2b, val_losses_2b, "Training vs Validation Loss (2.b - With One-Hot Encoding)")
plot_results(train_losses_2c, val_losses_2c, "Training vs Validation Loss (2.c - Increased Complexity)")

# Print Model Complexities
print(f"\nModel Complexity (Parameters):")
print(f"2.a: {sum(p.numel() for p in model_2a.parameters() if p.requires_grad)} parameters")
print(f"2.b: {sum(p.numel() for p in model_2b.parameters() if p.requires_grad)} parameters")
print(f"2.c: {sum(p.numel() for p in model_2c.parameters() if p.requires_grad)} parameters")
